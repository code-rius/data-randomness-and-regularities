@book{priemer1990introductory,
  title     = {Introductory Signal Processing},
  author    = {Priemer, R.},
  isbn      = {9789813103757},
  series    = {Advanced Series In Electrical And Computer Engineering},
  url       = {https://books.google.lt/books?id=5AM8DQAAQBAJ},
  year      = {1990},
  publisher = {World Scientific Publishing Company}
}

@book{lyons2004understanding,
  title     = {Understanding Digital Signal Processing},
  author    = {Lyons, R.G.},
  isbn      = {9780131089891},
  lccn      = {2004000990},
  series    = {Prentice Hall professional technical reference},
  url       = {https://books.google.lt/books?id=f\_yCQgAACAAJ},
  year      = {2004},
  publisher = {Prentice Hall/PTR}
}

@book{lathi1998signal,
  title     = {Signal Processing and Linear Systems},
  author    = {Lathi, B.P.},
  isbn      = {9780195219173},
  lccn      = {98010609},
  series    = {Oxford series in electrical and computer engineering},
  url       = {https://books.google.lt/books?id=6gy5wAEACAAJ},
  year      = {1998},
  publisher = {Oxford University Press}
}

@misc{recurrence_plot_tk, 
  title		  = {Recurrence Plots and Cross Recurrence Plots},	
  url 	  	= {http://recurrence-plot.tk/online/index.php},
  howpublished = {\newline{}http://recurrence-plot.tk/online/index.php},
  journal	  = {RECURRENCE PLOTS::Recurrence Plots, Cross Recurrence Plots, Dynamical Systems, Recurrence Quantification Analysis (RP, CRP, RQA), Recurrence Analysis, Nonlinear Dynamics, Chaos, Time series analysis, Data analysis}, 
  note      = {[Online; accessed 2-Jan-2021]},
  author	  = {Marwan, Norbert}
}

@misc{javascript_vs_python_ml,
  title     = {Performance Comparison: Javascript vs. Python for Machine Learning},
  url       = {https://dlabs.ai/blog/performance-comparison-javascript-vs-python-for-machine-learning/},
  howpublished = {\newline{}https://dlabs.ai/blog/performance-comparison-javascript-vs-python-for-machine-learning/},
  year      = {2020},
  note      = {[Online; accessed 2-Jan-2021]},
  author    = {Krzysztof, Miśtal}
}

@misc{react_home,
  title     = {React is a JavaScript library for building user interfaces},
  url       = {https://github.com/facebook/react},
  howpublished = {\newline{}https://github.com/facebook/react},
  year      = {2020},
  note      = {[Online; accessed 2-Jan-2021]},
  author    = {Facebook Inc.}
}

@misc{npm,
  title        = {NPM - Node Package Manager},
  url          = {https://github.com/npm/},
  howpublished = {\newline{}https://github.com/npm/},
  year         = {2021},
  note         = {[Online; accessed 2-Jan-2021]}
}

@misc{flask,
  title        = {Flask - Python web application framework},
  url          = {https://flask.palletsprojects.com/en/1.1.x/},
  howpublished = {\newline{}https://flask.palletsprojects.com/en/1.1.x/},
  year         = {2021},
  note         = {[Online; accessed 2-Jan-2021]}
}

@misc{express,
  title        = {Expressjs - NodeJS web application frameowrk},
  url          = {https://github.com/expressjs/expressjs.com},
  howpublished = {\newline{}https://github.com/expressjs/expressjs.com},
  year         = {2021},
  note         = {[Online; accessed 2-Jan-2021]}
}

@misc{node-fetch,
  title        = {Node Fetch - a module that brings window.fetch to Node.js},
  url          = {https://github.com/node-fetch/node-fetch},
  howpublished = {\newline{}https://github.com/node-fetch/node-fetch},
  year         = {2021},
  note         = {[Online; accessed 2-Jan-2021]}
}

@misc{query-string,
  title        = {Query-String - an HTTP query string building module},
  url          = {https://github.com/sindresorhus/query-string},
  howpublished = {\newline{}https://github.com/sindresorhus/query-string},
  year         = {2021},
  note         = {[Online; accessed 3-Jan-2021]}
}

@misc{sass,
  title        = {Sass - a preprocessor scripting language that is interpreted or compiled into Cascading Style Sheets},
  url          = {https://github.com/sass},
  howpublished = {\newline{}https://github.com/sass},
  year         = {2021},
  note         = {[Online; accessed 3-Jan-2021]}
}

@misc{mongoose,
  title        = {Mongoose - a MongoDB object modeling tool designed to work in an asynchronous environment},
  url          = {https://github.com/Automattic/mongoose},
  howpublished = {\newline{}https://github.com/Automattic/mongoose},
  year         = {2021},
  note         = {[Online; accessed 3-Jan-2021]}
}

@misc{numpy,
  title        = {NumPy - the fundamental package needed for scientific computing with Python},
  url          = {https://github.com/numpy/numpy},
  howpublished = {\newline{}https://github.com/numpy/numpy},
  year         = {2021},
  note         = {[Online; accessed 3-Jan-2021]}
}

@misc{scipy,
  title        = {SciPy (pronounced "Sigh Pie") is open-source software for mathematics, science, and engineering},
  url          = {https://github.com/scipy/scipy},
  howpublished = {\newline{}https://github.com/scipy/scipy},
  year         = {2021},
  note         = {[Online; accessed 3-Jan-2021]}
}

@misc{matplotlib,
  title        = {Matplotlib - a comprehensive library for creating static, animated, and interactive visualizations in Python.},
  url          = {https://github.com/matplotlib/matplotlib},
  howpublished = {\newline{}https://github.com/matplotlib/matplotlib},
  year         = {2021},
  note         = {[Online; accessed 3-Jan-2021]}
}

@misc{scikit-learn,
  title        = {Matplotlib - a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license.},
  url          = {https://github.com/scikit-learn/scikit-learn},
  howpublished = {\newline{}https://github.com/scikit-learn/scikit-learn},
  year         = {2021},
  note         = {[Online; accessed 4-Jan-2021]}
}

@misc{tensorflow,
  title        = {TensorFlow - an end-to-end open source platform for machine learning.},
  url          = {https://github.com/tensorflow/},
  howpublished = {\newline{}https://github.com/tensorflow/},
  year         = {2021},
  note         = {[Online; accessed 4-Jan-2021]}
}

@misc{keras,
  title        = {Keras - an API designed for human beings, not machines},
  url          = {https://github.com/keras-team/keras},
  howpublished = {\newline{}https://github.com/keras-team/keras},
  year         = {2021},
  note         = {[Online; accessed 4-Jan-2021]}
}

@misc{dropout,
  title        = {Keras dropout layer - Applies Dropout to the input},
  url          = {https://keras.io/api/layers/regularization\_layers/dropout/},
  howpublished = {\newline{}https://keras.io/api/layers/regularization\_layers/dropout/},
  year         = {2021},
  note         = {[Online; accessed 4-Jan-2021]}
}

@misc{sequential,
  title        = {Keras sequential groups a linear stack of layers into a tf.keras.Model.},
  url          = {https://keras.io/api/models/sequential/},
  howpublished = {\newline{}https://keras.io/api/models/sequential/},
  year         = {2021},
  note         = {[Online; accessed 4-Jan-2021]}
}

@article{brusilovsky:simonyan2014very,
  abstract             = {The used CNN implementation (VGGNet) by the main paper (used)},
  added-at             = {2020-06-08T19:18:30.000+0200},
  author               = {Simonyan, Karen and Zisserman, Andrew},
  biburl               = {https://www.bibsonomy.org/bibtex/2bc3ee27a1dd159f48b10ac3555879865/buch_jon},
  citeulike-article-id = {14183517},
  interhash            = {4e6fa56cb7cf99400d5701543ee228de},
  intrahash            = {bc3ee27a1dd159f48b10ac3555879865},
  journal              = {arXiv preprint arXiv:1409.1556},
  keywords             = {},
  posted-at            = {2016-11-14 05:20:24},
  priority             = {2},
  timestamp            = {2020-06-08T19:18:30.000+0200},
  title                = {{Very deep convolutional networks for large-scale image recognition}},
  year                 = 2014
}

@article{relu,
  title    = {A comparison of deep networks with ReLU activation function and linear spline-type methods},
  journal  = {Neural Networks},
  volume   = {110},
  pages    = {232 - 242},
  year     = {2019},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/j.neunet.2018.11.005},
  url      = {http://www.sciencedirect.com/science/article/pii/S0893608018303277},
  author   = {Konstantin Eckle and Johannes Schmidt-Hieber},
  keywords = {Deep neural networks, Nonparametric regression, Splines, MARS, Faber–Schauder system, Rates of convergence},
  abstract = {Deep neural networks (DNNs) generate much richer function spaces than shallow networks. Since the function spaces induced by shallow networks have several approximation theoretic drawbacks, this explains, however, not necessarily the success of deep networks. In this article we take another route by comparing the expressive power of DNNs with ReLU activation function to linear spline methods. We show that MARS (multivariate adaptive regression splines) is improper learnable by DNNs in the sense that for any given function that can be expressed as a function in MARS with M parameters there exists a multilayer neural network with O(Mlog(M∕ε)) parameters that approximates this function up to sup-norm error ε. We show a similar result for expansions with respect to the Faber–Schauder system. Based on this, we derive risk comparison inequalities that bound the statistical risk of fitting a neural network by the statistical risk of spline-based methods. This shows that deep networks perform better or only slightly worse than the considered spline methods. We provide a constructive proof for the function approximations.}
}

@article{recurrence_plot_tk_2,
  title     = "Recurrence Plots for the Analysis of Complex Systems",
  abstract  = "Recurrence is a fundamental property of dynamical systems, which can be exploited to characterise the system's behaviour in phase space. A powerful tool for their visualisation and analysis called recurrence plot was introduced in the late 1980's. This report is a comprehensive overview covering recurrence based methods and their applications with an emphasis on recent developments. After a brief outline of the theory of recurrences, the basic idea of the recurrence plot with its variations is presented. This includes the quantification of recurrence plots, like the recurrence quantification analysis, which is highly effective to detect, e.g., transitions in the dynamics of systems from time series. A main point is how to link recurrences to dynamical invariants and unstable periodic orbits. This and further evidence suggest that recurrences contain all relevant information about a system's behaviour. As the respective phase spaces of two systems change due to coupling, recurrence plots allow studying and quantifying their interaction. This fact also provides us with a sensitive tool for the study of synchronisation of complex systems. In the last part of the report several applications of recurrence plots in economy, physiology, neuroscience, earth sciences, astrophysics and engineering are shown. The aim of this work is to provide the readers with the know how for the application of recurrence plot based methods in their own field of research. We therefore detail the analysis of data and indicate possible difficulties and pitfalls.",
  keywords  = "data analysis, recurrence plot, nonlinear dynamics, time-series analysis, chaotic electrochemical oscillators, quantification analysis, phase synchronization, strange attractors, dynamical-systems, generalized synchronization, poincare recurrences, periodic-orbits, correlation dimension",
  author    = "Norbert Marwan and Romano, {M Carmen} and Marco Thiel and Jurgen Kurths",
  year      = "2007",
  month     = jan,
  day       = "12",
  doi       = "10.1016/J.PHYSREP.2006.11.001",
  language  = "English",
  volume    = "438",
  pages     = "237--329",
  journal   = "Physics Reports",
  issn      = "0370-1573",
  publisher = "Elsevier",
  number    = "5-6"
  }

@book{krause2015microservices,
  title     = {Microservices: Patterns and Applications: Designing Fine-Grained Services by Applying Patterns},
  author    = {Krause, L.},
  isbn      = {9780692424278},
  url       = {https://books.google.lt/books?id=dd5-rgEACAAJ},
  year      = {2015},
  publisher = {Lucas Krause}
}

@article{recent_advances_in_cnn,
  title    = {Recent advances in convolutional neural networks},
  journal  = {Pattern Recognition},
  volume   = {77},
  pages    = {354 - 377},
  year     = {2018},
  issn     = {0031-3203},
  doi      = {https://doi.org/10.1016/j.patcog.2017.10.013},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320317304120},
  author   = {Jiuxiang Gu and Zhenhua Wang and Jason Kuen and Lianyang Ma and Amir Shahroudy and Bing Shuai and Ting Liu and Xingxing Wang and Gang Wang and Jianfei Cai and Tsuhan Chen},
  keywords = {Convolutional neural network, Deep learning},
  abstract = {In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.}
}

@inproceedings{understanding_cnn,
  author    = {S. {Albawi} and T. A. {Mohammed} and S. {Al-Zawi}},
  booktitle = {2017 International Conference on Engineering and Technology (ICET)},
  title     = {Understanding of a convolutional neural network},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {1-6},
  doi       = {10.1109/ICEngTechnol.2017.8308186}
}

@article{overfitting,
  doi       = {10.1088/1742-6596/1168/2/022022},
  url       = {https://doi.org/10.1088/1742-6596/1168/2/022022},
  year      = 2019,
  month     = {feb},
  publisher = {{IOP} Publishing},
  volume    = {1168},
  pages     = {022022},
  author    = {Xue Ying},
  title     = {An Overview of Overfitting and its Solutions},
  journal   = {Journal of Physics: Conference Series},
  abstract  = {Overfitting is a fundamental issue in supervised machine learning which prevents us from perfectly generalizing the models to well fit observed data on training data, as well as unseen data on testing set. Because of the presence of noise, the limited size of training set, and the complexity of classifiers, overfitting happens. This paper is going to talk about overfitting from the perspectives of causes and solutions. To reduce the effects of overfitting, various strategies are proposed to address to these causes: 1) “early-stopping” strategy is introduced to prevent overfitting by stopping training before the performance stops optimize; 2) “network-reduction” strategy is used to exclude the noises in training set; 3) “data-expansion” strategy is proposed for complicated models to fine-tune the hyper-parameters sets with a great amount of data; and 4) “regularization” strategy is proposed to guarantee models performance to a great extent while dealing with real world issues by feature-selection, and by distinguishing more useful and less useful features.}
}

@misc{kingma2017adam,
  title         = {Adam: A Method for Stochastic Optimization},
  author        = {Diederik P. Kingma and Jimmy Ba},
  year          = {2017},
  eprint        = {1412.6980},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}